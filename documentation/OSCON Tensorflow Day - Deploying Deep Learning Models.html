
<!DOCTYPE html>
<html>
  <head>
    <title>Deploying Deep Learning Models</title>
    <meta charset="utf-8" />

    <style>
      @import url(https://fonts.googleapis.com/css?family=Droid+Serif);
      @import url(https://fonts.googleapis.com/css?family=Yanone+Kaffeesatz);

      body {
        font-family: 'Droid Serif';
        font-size: medium;
      }
      h1, h2, h3, h4 {
        font-family: 'Yanone Kaffeesatz';
        font-weight: 400;
        margin-bottom: 0;
      }
      .small * {
        font-size: small !important;
      }
      code {
        border-radius: 5px;
      }
      .inverse {
        background: #54A6F7;
        /* background: #272822; */
        color: #777872;
        /* text-shadow: 0 0 20px #333; */
      }
      .inverse h1, .inverse h2, .inverse h3, .inverse h4 {
        color: #f3f3f3;
        line-height: 1em;
      }
      .inverse h3, .inverse h2 {
        color: #f3f3f3;
        line-height: 1em;
      }
      .remark-slide-number {
          font-family: 'Yanone Kaffeesatz';
          font-size: 10pt;
          margin-bottom: 0px;
          margin-right: 10px;
          color: #000000; 
          opacity: 1; /* default: 0.5 */
      }
      .footnote {
        position: absolute;
        font-family: 'Yanone Kaffeesatz';
        font-size: small;
        bottom: 3em;
        right: 3em;
      }
      /* styling only necessary for displaying source */
      #source {
        position: absolute;
        display: none;
        font-family: monospace;
        font-size: medium;
        background: #333333;
        color: white;
        padding: 10px;
        text-align: left;
        width: 65%;
        height: 70%;
        z-index: 1000;
      }
      #overlay {
        position: absolute;
        display: none;
        background: black;
        width: 100%;
        height: 100%;
        opacity: 0.2;
        z-index: 999;
      }
    </style>
  </head>
  <body>
    <textarea id="source" readonly>

class: center, middle, inverse

# Deploying Deep Learning Models

## OSCON Tensorflow Day 2018

### Hannes Hapke

#### @hanneshapke

.footnote[
  Slides available at https://github.com/hanneshapke/Deploying_Deep_Learning_Models 
]

---
class: center, middle, inverse

# Does the following scenario sound familiar?

---
class: inverse

### Joe (data scientist): Hey Jane, my model is validated and tested. I would like to deploy it. 
### Jane (back-end engineer): Great, do you have an API for it? 
--

### Joe: API? Our model runs on TF/Python. The entire back-end runs on Ruby. I haven’t written Ruby in years … 
### Jane: Ufff, I have never written Tensorflow code. Is that a Python library? 
--

### Joe: Hm, I guess, I’ll write some Ruby API code then.

---
class: center, middle

# What's the problem? 
---
class: center, middle

# Who owns the API?
---
class: center, middle

# Data science code deployed to API instances?
---
class: center, middle

# Different language expertises are needed
---
class: center, middle

# Coordinate release cycles between teams?
---
class: center, middle

# Coordination about model versioning
---

class: center, middle

# Hi, I'm Hannes.

#### Data Science Engineer at Cambia Health Solutions

---

# Agenda

* Requirements for Model Deployments

* Sample project

* How not to deploy models

* Deploying Models 

    * with Tensorflow Serving on premise

    * in the Cloud

    * with Kubeflow

* Other deployment options 

---

# Infrastructure Architectures

--

### Loading models on the backend server
.center[![No Model Server Architecture](images/no_model_server.png)]

--

### Using a model server
.center[![Model Server Architecture](images/model_server.png)]

---

# Model deployments should ...
--

### 1. Separate data science code from backend code

--

### 2. Reduce boilerplate code

--

### 3. Allow isolation of memory and CPU requirements

--

### 4. Support multiple models

--

### 5. Allow asyncronous requests

---

# Sample Project

### Model Structure

Let's predict Amazon product ratings based on the comments with a small LSTM network.
```python
model_input = Input(shape=(MAX_TOKENS,))
x = Embedding(input_dim=len(CHARS), output_dim=10, input_length=MAX_TOKENS)(model_input)
x = LSTM(128)(text_input)
output = Dense(5, activation='softmax')(x)
model = Model(inputs=text_input, outputs=output)
optimizer = RMSprop(lr=0.01)
model.compile(loss='categorical_crossentropy', optimizer=optimizer)
```

```python
model.fit(x_train, y_train, 
          batch_size=BATCH_SIZE, epochs=EPOCHS, 
          verbose=1, validation_data=(x_test, y_test),
          callbacks=[keras.callbacks.ModelCheckpoint('/tmp/amazon_ratings', 
                                                     monitor='val_loss', 
                                                     verbose=1, save_best_only=True,
                                                     save_weights_only=False, 
                                                     mode='auto', period=1)])
```

---
 
## Testing our Model

#### Negative Review

```python
>> test_sentence = "horrible book, don't buy it"
>> test_vector = clean_data(test_sentence, max_tokens=MAX_TOKENS, sup_chars=CHARS)
>> model.predict(test_vector.reshape(1, MAX_TOKENS, len(CHARS)))
[[0.5927979  0.23748466 0.10798287 0.03301411 0.02872046]]
```

#### Positive Review

```python
>> test_sentence = "Awesome product."
>> test_vector = clean_data(test_sentence, max_tokens=MAX_TOKENS, sup_chars=CHARS)
>> model.predict(test_vector.reshape(1, MAX_TOKENS, len(CHARS)))
[[0.03493131 0.0394276  0.08326671 0.2957105  0.5466638 ]]
```

---
class: center, middle, inverse

# How not to deploy a model ...

---

# Deploy with Flask + Keras

.small[
```python
@app.route("/predict", methods=["POST"])
def predict():
    # initialize the data dictionary that will be returned from the
    # view
    data = {"success": False}

    # ensure an image was properly uploaded to our endpoint
    if flask.request.method == "POST":
        if flask.request.files.get("image"):
            # read the image in PIL format
            image = flask.request.files["image"].read()
            image = Image.open(io.BytesIO(image))

            # preprocess the image and prepare it for classification
            image = prepare_image(image, target=(224, 224))

            # classify the input image and then initialize the list
            # of predictions to return to the client
            preds = model.predict(image)
            results = imagenet_utils.decode_predictions(preds)
            data["predictions"] = []

            # loop over the results and add them to the list of
            # returned predictions
            for (imagenetID, label, prob) in results[0]:
                r = {"label": label, "probability": float(prob)}
                data["predictions"].append(r)

            # indicate that the request was a success
            data["success"] = True

    # return the data dictionary as a JSON response
    return flask.jsonify(data)
```
]

.footnote[
  Code snippet from [Keras Blog](https://blog.keras.io/building-a-simple-keras-deep-learning-rest-api.html)
]
---
class: center, middle, inverse

# Don't deploy that way if can avoid it.
---

## Why? 

--

### 1. Mix of data science and backend code

--

### 2. Boilerplate API code

--

### 3. API instances need enough memory to load models

--

### 4. Multiple models?

--

### 5. No asynchronous requests

---
class: center, middle, inverse

# Use Tensorflow Serving instead.
---

class: center, middle, inverse

# But before that, let's chat about some terms.
---

# Important Terms 

### Protocol Buffers
Protobufs are a method of serializing structured data. Binary format. 

### Bazel
Automation tool to build software. Similar to Make or Apache Maven.

### gRPC
(Google) Remote Procedure Call. HTTP/2 based. Uses ProtoBuf. 

### REST
Representational State Transfer. Architectural style for web services.  

---
class: center, middle, inverse

# Welcome Tensorflow Serving!
---
# Steps to deploy a model
 
--

### 1. Export model structure weights, as well as model signatures as protobuf

--

### 2. Set up the Tensorflow Server

--

### 3. Create a gRPC client

--

### 4. Load the model

---

## Export our Keras model to Protobuf

```python
import os
from keras import backend as K
import tensorflow as tf

tf.app.flags.DEFINE_integer('training_iteration', 1000, 'number of training iterations.')
tf.app.flags.DEFINE_integer('model_version', 1, 'version number of the model.')
tf.app.flags.DEFINE_string('work_dir', '/tmp', 'Working directory.')
FLAGS = tf.app.flags.FLAGS

export_path_base = '/tmp/amazon_reviews'
export_path = os.path.join(tf.compat.as_bytes(export_path_base), 
			   tf.compat.as_bytes(str(FLAGS.model_version)))

builder = tf.saved_model.builder.SavedModelBuilder(export_path)

signature = tf.saved_model.signature_def_utils.predict_signature_def(
    inputs={'input': model.input}, outputs={'rating_prob': model.output})

builder.add_meta_graph_and_variables(
    sess=K.get_session(), tags=[tf.saved_model.tag_constants.SERVING],
    signature_def_map={
        tf.saved_model.signature_constants.DEFAULT_SERVING_SIGNATURE_DEF_KEY: signature })
builder.save()

```
---
class: center, middle, inverse

# Let's unpack what we just saw.
---

# Flags

* Let's set flags with information relevant for the model

```python
import tensorflow as tf

tf.app.flags.DEFINE_integer('training_iteration', 1000, 
                            'number of training iterations.')
tf.app.flags.DEFINE_integer('model_version', 1, 
                            'version number of the model.')
tf.app.flags.DEFINE_string('work_dir', '/tmp', 
                           'Working directory.')
FLAGS = tf.app.flags.FLAGS
```

???
Using Abseil py package

---

# Model Signatures

* Tensorflow Serving requires that every model has a model signature

* Signature define the generic _inputs_ and _outputs_ of a function

```python
signature = tf.saved_model.signature_def_utils.predict_signature_def(
    inputs={model_name + '_input': model.input}, outputs={model_name + '_output': model.output})

builder.add_meta_graph_and_variables(
      sess=K.get_session(), tags=[tf.saved_model.tag_constants.SERVING],
      signature_def_map={
          tf.saved_model.signature_constants.DEFAULT_SERVING_SIGNATURE_DEF_KEY:
              signature
      })
```
---

# Exporting the Model

* The _SavedModelBuilder_ will export your model to a predefined ProtoBuf format

```python
export_path_base = '/tmp/amazon_reviews'
export_path = os.path.join(
      tf.compat.as_bytes(export_path_base),
      tf.compat.as_bytes(str(FLAGS.model_version)))

print('Exporting trained model to', export_path)

builder = tf.saved_model.builder.SavedModelBuilder(export_path)
...
builder.save()

```
---
class: center, middle, inverse

# Now you have exported your model.
---

## Exported Models

* You should find these protobuf files in your folder structure

.center[![Exported model](images/folders.png)]

* The files should include a 

    * saved_model.pb

    * variable.index 

    * one or more variable.data* files.

???
assets is a subfolder containing auxiliary (external) files, such as vocabularies. Assets are copied to the SavedModel location and can be read when loading a specific MetaGraphDef.

assets.extra is a subfolder where higher-level libraries and users can add their own assets that co-exist with the model, but are not loaded by the graph. This subfolder is not managed by the SavedModel libraries.

variables is a subfolder that includes output from tf.train.Saver.

saved_model.pb or saved_model.pbtxt is the SavedModel protocol buffer. It includes the graph definitions as MetaGraphDef protocol buffers.

---

class: center, middle, inverse

# Let's set up your Tensorflow server
---

# Creating a Tensorflow Serving Environment

* If you need optimizations, clone the TF Serving repo and build your server with Bazel

* Otherwise install Tensorflow server in a Docker container


```terminal
$ git clone git@github.com:hanneshapke/Deploying_Deep_Learning_Models.git

$ docker build --pull -t $USER/tensorflow-serving-devel-cpu \
                      -f {path to repo}/\
                      Deploying_Deep_Learning_Models/\
                      examples/Dockerfile .
```

---

# Starting up the Server

* Start up the container with 

```terminal
$ docker run -it -p 8500:8500 -p 8501:8501 
             -v {model_path}/exported_models/amazon_review/:/models 
             $USER/tensorflow-serving-devel-cpu:latest /bin/bash
```

???

-t              : Allocate a pseudo-tty
-i              : Keep STDIN open even if not attached

---

## What's happening inside the Docker container?

* Starting up the Tensorflow Serving instance

```terminal
$[docker bash] tensorflow_model_server --port=8500 
                                       --model_name={model_name}
                                       --model_base_path=/models/{model_name}
```

--

* This should generate similar output like this 

.small[
```terminal
2018-06-29 00:02:05.611608: 
    tensorflow_serving/model_servers/server_core.cc:444 Adding/updating models.
2018-06-29 00:02:05.611712: 
    tensorflow_serving/model_servers/server_core.cc:499  (Re-)adding model: amazon_review
2018-06-29 00:02:05.729657: 
    tensorflow_serving/core/basic_manager.cc:716 
    Successfully reserved resources to load servable {name: amazon_review version: 2}
2018-06-29 00:02:05.729731: 
    tensorflow_serving/core/loader_harness.cc:66 
    Approving load for servable version {name: amazon_review version: 2}
2018-06-29 00:02:05.729761: 
    tensorflow_serving/core/loader_harness.cc:74 
    Loading servable version {name: amazon_review version: 2}
...
2018-06-29 00:02:05.855197: 
    tensorflow_serving/core/loader_harness.cc:86 
    Successfully loaded servable version {name: amazon_review version: 2}
2018-06-29 00:02:05.863820: 
    tensorflow_serving/model_servers/main.cc:323 Running ModelServer at 0.0.0.0:8500 ...
2018-06-29 00:02:05.870805: 
    tensorflow_serving/model_servers/main.cc:333 Exporting HTTP/REST API at:localhost:8501 ...
evhttp_server.cc : 235 RAW: Entering the event loop ...
```
]

---

## Useful tips

### Inspect your models 

```terminal
$[docker bash] saved_model_cli show --dir=/models/{model_name}/{version_number}
                                    --tag_set serve 
                                    --signature_def serving_default
```

---

# Tensorflow Serving Client

## Dependecies

- tensorflow_serving

- grpc

```terminal
$ pip install tensorflow-serving-api grpc
```

.footnote[
  Using Python 3? [Python 3 compliance](https://github.com/tensorflow/serving/pull/685/files) [Fix Python 3 issues](https://github.com/tensorflo
w/serving/issues/700)
]

---

# Tensorflow Serving Client

## Connecting to the RPC host

```python
from grpc.beta import implementations
from tensorflow_serving.apis import prediction_service_pb2

def get_stub(host='127.0.0.1', port='8500'):
    channel = implementations.insecure_channel(host, int(port))
    stub = prediction_service_pb2.beta_create_PredictionService_stub(channel)
    return stub
```

---

# Tensorflow Serving Client

## Request prediction using gRPC

* Very barebone implementation!

```python
def get_model_prediction(model_input, stub, 
                         model_name='amazon_reviews', 
                         signature_name='serving_default'):

    request = predict_pb2.PredictRequest()
    request.model_spec.name = model_name
    request.model_spec.signature_name = signature_name
    
    request.inputs['input'].CopyFrom(
        tf.contrib.util.make_tensor_proto(
            model_input.reshape(1, 50), 
            verify_shape=True, shape=(1, 50)))
 
    response = stub.Predict.future(request, 5.0)  # wait max 5s
    return response.result().outputs["rating_prob"].float_val
```

---

# Tensorflow Serving Client

## Request prediction using gRPC

* Very barebone implementation!


```python
>>> sentence = "this product is really helpful"
>>> model_input = clean_data_encoded(sentence)

>>> get_model_prediction(model_input, stub)
[0.0250927172601223, 0.03738045319914818, 0.09454590082168579, 
0.33069494366645813, 0.5122858881950378]
```

---

# Tensorflow Serving Client

## Request prediction from a specific model version

* You can specify the specific model version

* If no model version is provided, TF Serving loads the model with the latest model version

```python
request = predict_pb2.PredictRequest()
request.model_spec.name = 'amazon_reviews'
request.model_spec.version.value = 1
```

---

# Tensorflow Serving Client

## Obtain model meta data

```python
def get_model_meta(model_name, stub):
    request = get_model_metadata_pb2.GetModelMetadataRequest()
    request.model_spec.name = model_name
    request.metadata_field.append("signature_def")
    response = stub.GetModelMetadata(request, 5)
    return response.metadata['signature_def']

>>> meta = get_model_meta(model_name, stub)
>>> print(meta.SerializeToString().decode("utf-8", 'ignore'))
type.googleapis.com/tensorflow.serving.SignatureDefMap
serving_default
amazon_review_input
        input_1:0
               2@
amazon_review_output(
dense_1/Softmax:0
               tensorflow/serving/predict

```

---

# Tensorflow Serving Client

## Obtain model version

```python
def get_model_version(model_name, stub):
    request = get_model_metadata_pb2.GetModelMetadataRequest()
    request.model_spec.name = model_name
    request.metadata_field.append("signature_def")
    response = stub.GetModelMetadata(request, 5)
    return response.model_spec.version.value
```

```python
>>> model_name = 'amazon_review'
>>> stub = get_stub()

>>> get_model_version(model_name, stub)
2L
```
---

# Tensorflow Serving Client using REST

* Tensorflow Serving supports REST requests since release 1.8

```terminal
$[docker bash] tensorflow_model_server --port=8500 
                                       --rest_api_port=8501 
                                       --model_name={model_name}
                                       --model_base_path=/models/{model_name}
```
---

# Tensorflow Serving Client using REST

* The URI should be

    * `http://host:port/<URI>:<VERB>`

    * URI `/v1/models/{model_name}/versions/{model_version}`

    * verb `classify|regress|predict`

* Use the Python _requests_ package for REST calls.

```python
def get_model_prediction(model_input, model_name='amazon_review', signature_name='serving_default'):
    url = get_rest_url(model_name)
    data = {"instances": [model_input.tolist()]}

    rv = requests.post(url, data=json.dumps(data))
    if rv.status_code != requests.codes.ok:
        rv.raise_for_status()
    
    return rv.json()['predictions']
```

---

# Tensorflow Serving Client using REST

.center[![REST requests](images/postman.png)]

---

# Tensorflow Serving

## Serve multiple models

* Provide a server config file _config.file_

```json
model_config_list: {
    config:{
        name:"amazon_reviews",
        base_path:"/models/{model_name}",
        model_platform:"tensorflow",
        model_version_policy: { all: {} }
    },
    config:{
        name:"amazon_ratings",
        base_path:"/models/{other_model_name}",
        model_platform:"tensorflow",
        model_version_policy: { all: {} }
    }
}
```

---

# Tensorflow Serving

## Serve multiple models

* Start the server using config file

Instead of 

```terminal
$ tensorflow_model_server --port=8500 
                          --model_name={model_name}
                          --model_base_path=/models/{model_name}
```

use 

```terminal
$ tensorflow_model_server --port=8500
                          --model_config_file=/path/to/\
                            config/file.config
```

---

# How to do A/B Testing?

* Easily possible since multiple versions can be served

* A/B testing of models can be performed by selecting the models from the client side

* Set the specific version in your gRPC or REST request

.footnote[
    [Discussion on the TF.org mailing list](https://groups.google.com/a/tensorflow.org/forum/#!topic/discuss/bn3_I640R5E)
]

```python
from random import random
def get_rest_url(model_name, host='127.0.1', port='8501', 
                 verb='predict', version=None):
    url = "http://{host}:{port}/v1/models/{model_name}".format(
        host=host, port=port, model_name=model_name)
    if version:
        url += 'versions/{version}'.format(version=version)
    url += ':{verb}'.format(verb=verb)
    return url

# 10% of requests to the latest model
version = 1 if random() > 0.1 else None
url = get_rest_url('amazon_review', version=version)
```

---

## Good idea? 

--

1. No mix of data science and backend code

--

2. No boilerplate API code

--

3. APIs can be serverless

--

4. Multiple models? Of course.

--

5. Asyncronous requests. Heck yes!

---

# Serving Models via the Cloud 

* Exported Tensorflow or Keras models can be served via the _Google Cloud Platform_ and [Google Cloud ML Engine](https://cloud.google.com/ml-engine/)

* Detailed information on [GCP Model Deployments](https://cloud.google.com/ml-engine/docs/tensorflow/deploying-models)

.center[![Google Cloud ML Engine](images/gcp_demo.png)]

---

# Serving Models via Google Cloud ML Engine

* Copy the exported model to a Google storage bucket

```terminal
$ gsutil cp -r inception gs://<bucket-name>
$ gsutil ls -r gs://<bucket-name>
gs://<bucket-name>/amazon_review/:

gs://<bucket-name>/amazon_review/1/:
gs://<bucket-name>/amazon_review/1/saved_model.pb
...
```

* Create a json file with the request data

```terminal
$ MODEL_NAME="tf_serving_demo"
$ INPUT_DATA_FILE="request_data.json"
$ VERSION_NAME="amazon_review_prediction"
$ gcloud ml-engine predict --model $MODEL_NAME  \
                           --version $VERSION_NAME \
                           --json-instances $INPUT_DATA_FILE
```

.footnote[
    install of gsutils: https://cloud.google.com/storage/docs/gsutil_install
]

???
 hannes@Hanness-MBP-2  ~  gsutil cp -r ~/Development/opensource/Deploying_Deep_Learning_Models/examples/export_keras_model/exported_models/amazon_review gs://model-server
Copying file:///Users/hannes/Development/opensource/Deploying_Deep_Learning_Models/examples/export_keras_model/exported_models/amazon_review/1/saved_model.pb [Content-Type=application/octet-stream]...
Copying file:///Users/hannes/Development/opensource/Deploying_Deep_Learning_Models/examples/export_keras_model/exported_models/amazon_review/1/variables/variables.data-00000-of-00001 [Content-Type=application/octet-stream]...
Copying file:///Users/hannes/Development/opensource/Deploying_Deep_Learning_Models/examples/export_keras_model/exported_models/amazon_review/1/variables/variables.index [Content-Type=application/octet-stream]...
- [3 files][198.6 KiB/198.6 KiB]
Operation completed over 3 objects/198.6 KiB.

<!---

# Kubeflow for all

# More ways to deploy Deep Learning Models

## Seldon

## MLflow

--->

---

# Conclusion

--

* Reasons to serve models with Tensorflow Serving
    
    * Separates data science code from API code
    
    * No boilerplate code
    
    * Can handle multiple models and versions

--

* Steps to deploy
    
    * Export your model
    
    * Setup your server
    
    * Request predictions via gRPC or REST

---

class: center, middle, inverse

# Thank you and happy deploying!

### @hanneshapke




    </textarea>

    <script src="https://remarkjs.com/downloads/remark-latest.min.js"></script>
    <script type="text/javascript">
      var hljs = remark.highlighter.engine;
    </script>
    <script src="terminal.language.js" type="text/javascript"></script>
    <script type="text/javascript">
      var slideshow = remark.create({
        highlightStyle: 'monokai',
	ratio: '16:9',
        // Customize slide number label, either using a format string..
        slideNumberFormat: 'Slide %current% of %total%',
        // .. or by using a format function
        slideNumberFormat: function (current, total) {
            return Math.round((1 - current / total) * 100)  + '% remaining';
        },
      });
      // extract the embedded styling from ansi spans
      var highlighted = document.querySelectorAll("code.terminal span.hljs-ansi");
      Array.prototype.forEach.call(highlighted, function(next) {
        next.insertAdjacentHTML("beforebegin", next.textContent);
        next.parentNode.removeChild(next);
      });
    </script>
    
  </body>
</html>

<!--
  vim:filetype=markdown
-->
